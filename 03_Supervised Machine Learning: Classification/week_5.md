# WEEK 5 QUIZ

## Q1. The term Bagging stands for bootstrap aggregating.
`True`

## Q2. This is the best way to choose the number of trees to build on a Bagging ensemble.

Tune number of trees as a hyperparameter that needs to be optimized

Prioratize training error metrics over out of bag sample

Choose a number of trees past the point of diminishing returns

## Q3. Which type of Ensemble modeling approach is NOT a special case of model averaging?

The Bagging method of Bootstrap aggregation


Random Forest methods


Boosting methods


## Q4. What is an ensemble model that needs you to look at out of bag error?
`Random Forest`

## Q5. What is the main condition to use stacking as ensemble method?

Models need to output predicted probabilities

## Q6. This tree ensemble method only uses a subset of the features for each tree:

`Random Forest`

## Q7. Order these tree ensembles in order of most randomness to least randomness:
Random Trees, Random Forest, Bagging

## Q8. This is an ensemble model that does not use bootstrapped samples to fit the base trees, takes residuals into account, and fits the base trees iteratively:
`Boosting`

## Q9. When comparing the two ensemble methods Bagging and Boosting, what is one characteristic of Boosting?
`Fits entire data set`

## Q10. What is the most frequently discussed loss function in boosting algorithms?

0-1 Loss Function


Gradient Loss Function


Gradient Boosting Loss Function
